This development is possible due to **Transformer** architecture. It helps to understand large language text inputs.

![image](https://github.com/user-attachments/assets/c365004d-d2f5-4297-a1e8-3bc80d0f4a35)


![image](https://github.com/user-attachments/assets/2c8441a4-7c25-461b-aad0-1efd410ee8cf)




### All foundation models are LLM but all LLMs are not foundation model

![image](https://github.com/user-attachments/assets/2e89d455-564c-4610-a8ae-a77aa74f3739)


